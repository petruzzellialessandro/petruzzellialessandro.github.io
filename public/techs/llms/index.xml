<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLMs on petruzzellialessandro</title>
    <link>http://example.org/techs/llms/</link>
    <description>Recent content in LLMs on petruzzellialessandro</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 07 Aug 2023 00:00:00 +0000</lastBuildDate><atom:link href="http://example.org/techs/llms/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Lora: Low-Rank Adaptation of Large Language Models</title>
      <link>http://example.org/extra/lora/</link>
      <pubDate>Mon, 07 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/extra/lora/</guid>
      <description></description>
    </item>
    
    <item>
      <title>M6-Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems</title>
      <link>http://example.org/extra/m6rec/</link>
      <pubDate>Mon, 07 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/extra/m6rec/</guid>
      <description></description>
    </item>
    
    <item>
      <title>P5: Recommendation as Language Processing (RLP)</title>
      <link>http://example.org/extra/p5/</link>
      <pubDate>Mon, 07 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/extra/p5/</guid>
      <description></description>
    </item>
    
    <item>
      <title>RecLLM: Leveraging Large Language Models in Conversational Recommender Systems</title>
      <link>http://example.org/extra/recllm/</link>
      <pubDate>Mon, 07 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/extra/recllm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Survey overview about LLMs in Recommender Systems</title>
      <link>http://example.org/portfolio/llms-in-recommendersystems/</link>
      <pubDate>Mon, 07 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/portfolio/llms-in-recommendersystems/</guid>
      <description>A July 2023 survey that analyzes the solutions (existing and non-existing) with which LLMs can be used to make recommendations.
Three ways of using LLMs are distinguished:
Pre-trained → Training an LLM on one or more recommender datasets. De facto, they are SLMs.
Fine-tuning → Refinement of an LLM for a specific purpose. Two techniques are distinguished:
Full-model Fine-tuning (FMFT): All model parameters are re-weighted. Lots of data and VRAM are required.</description>
    </item>
    
    <item>
      <title>TALLRec: Efficient Tuning Framework to Align LLM with Recommendation</title>
      <link>http://example.org/extra/tallrec/</link>
      <pubDate>Mon, 07 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/extra/tallrec/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
